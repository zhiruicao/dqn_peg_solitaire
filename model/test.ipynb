{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PegSolitaire:\n    def __init__(self):\n        self.board_layout = np.array([\n            [-1, -1,  1,  1,  1, -1, -1],\n            [-1, -1,  1,  1,  1, -1, -1],\n            [ 1,  1,  1,  1,  1,  1,  1],\n            [ 1,  1,  1,  0,  1,  1,  1],\n            [ 1,  1,  1,  1,  1,  1,  1],\n            [-1, -1,  1,  1,  1, -1, -1],\n            [-1, -1,  1,  1,  1, -1, -1]\n        ]).flatten()\n\n        self.actions = np.where(self.board_layout != -1)[0]\n        self.moves = self.get_moves()\n        self.reset(0, 1)\n\n    def get_moves(self):\n        moves = []\n        directions = [(-2, 0), (2, 0), (0, -2), (0, 2)]\n        for f in range(49):\n            if self.board_layout[f] == -1: continue\n            x, y = divmod(f, 7)\n            for dx, dy in directions:\n                tx, ty = x + dx, y + dy\n                if 0 <= tx < 7 and 0 <= ty < 7:\n                    t = tx * 7 + ty\n                    if self.board_layout[t] != -1:\n                        m = (x + dx//2) * 7 + y + dy//2\n                        moves.append([f, m, t])\n        return np.array(moves, dtype=np.int8)\n\n    def get_mask(self):\n        b = self.board\n        return (b[self.moves[:, 0]] == 1) & (b[self.moves[:, 1]] == 1) & (b[self.moves[:, 2]] == 0)\n\n    def step(self, action):\n        self.board[self.moves[action]] = [0, 0, 1]\n        self.remain -= 1\n        mask = self.get_mask()\n        self.done = not np.any(mask)\n        return self.board.copy(), 0, self.done, mask\n\n    def reset(self, steps, mode=0):\n        if mode == 0:\n            while True:\n                self.board = np.full(49, -1, dtype=np.int8)\n                for idx in self.actions: self.board[idx] = 0\n                self.board[24] = 1 \n                \n                actual_steps = 0\n                for _ in range(steps):\n                    b = self.board\n                    rev_mask = (b[self.moves[:, 0]] == 0) & (b[self.moves[:, 1]] == 0) & (b[self.moves[:, 2]] == 1)\n                    possible = np.where(rev_mask)[0]\n                    if len(possible) == 0: break \n                    idx = np.random.choice(possible)\n                    self.board[self.moves[idx, [0, 1, 2]]] = [1, 1, 0]\n                    actual_steps += 1\n                    \n                if actual_steps == steps: \n                    break\n        else:\n            while True:\n                self.board = self.board_layout.copy()\n                \n                actual_steps = 0\n                for _ in range(steps):\n                    mask = self.get_mask()\n                    actions = np.where(mask)[0]\n                    if len(actions) == 0: \n                        break\n                    idx = np.random.choice(actions)\n                    self.board[self.moves[idx]] = [0, 0, 1]\n                    actual_steps += 1\n                \n                if actual_steps == steps:\n                    break\n\n        self.remain = np.sum(self.board == 1)\n        self.done = False\n        return self.board.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Agent:\n    def __init__(self):\n        self.device = torch.device(\"cpu\") \n        self.q_network = DQN().to(self.device)\n        self.q_network.load_state_dict(torch.load('/kaggle/input/peg-solitaire-model-weights/model.pth', map_location=self.device))\n        self.q_network.eval() \n    def act(self, state, mask):\n        with torch.no_grad():\n            state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            q_values = self.q_network(state_t).cpu().numpy()[0]\n            \n            actions = np.where(mask)[0]\n            if len(actions) == 0:\n                return None, 0\n\n            best_idx = np.argmax(q_values[actions])\n            action = actions[best_idx]\n            \n            sorted_q = np.sort(q_values)[::-1]\n            chosen_q = q_values[action]\n            rank = np.where(sorted_q == chosen_q)[0][0]\n            \n            return action, rank","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DQN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.feature_layer = nn.Sequential(\n            nn.Linear(49, 128),\n            nn.ReLU(),\n        )\n        self.value_stream = nn.Sequential(\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n        self.advantage_stream = nn.Sequential(\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 76)\n        )\n\n    def forward(self, x):\n        features = self.feature_layer(x)\n        value = self.value_stream(features)\n        advantage = self.advantage_stream(features)\n        return value + advantage - advantage.mean(dim=-1, keepdim=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test(step, mode):\n    wins = 0\n    episodes = 1000\n    remains = []\n    q_ranks = []\n        \n    for _ in range(episodes):\n        state = env.reset(step, mode)\n        mask = env.get_mask()\n            \n        game_ranks = []\n            \n        while not env.done:\n            action, rank = agent.act(state, mask)\n            game_ranks.append(rank)\n                \n            state, _, done, mask = env.step(action)\n\n        if env.remain == 1 and env.board[24] == 1: \n            wins += 1\n        remains.append(env.remain)\n        if len(game_ranks) > 0:\n            q_ranks.append(np.mean(game_ranks))\n            \n    if mode == 1:\n        step = 31 - step\n        \n    print(f'Step {step:<3}  |  Win {wins/episodes:<6.1%}  |  Remain {np.mean(remains):<6.2f}  |  Q Rank {np.mean(q_ranks):.1f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"env = PegSolitaire()\nagent = Agent()\n\nprint('Reverse Test')\nfor step in range(1, 27):\n    test(step, 0)\n\nprint('Forward Test')\nfor step in range(1, 11):\n    test(step, 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}