{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bq1LYZhUQuy",
    "trusted": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nza1uf1xUQu0",
    "trusted": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class PegSolitaire:\n",
    "    def __init__(self):\n",
    "        self.board_layout = np.array([\n",
    "            [-1, -1,  1,  1,  1, -1, -1],\n",
    "            [-1, -1,  1,  1,  1, -1, -1],\n",
    "            [ 1,  1,  1,  1,  1,  1,  1],\n",
    "            [ 1,  1,  1,  0,  1,  1,  1],\n",
    "            [ 1,  1,  1,  1,  1,  1,  1],\n",
    "            [-1, -1,  1,  1,  1, -1, -1],\n",
    "            [-1, -1,  1,  1,  1, -1, -1]\n",
    "        ]).flatten()\n",
    "        \n",
    "        self.actions = np.where(self.board_layout != -1)[0]  # index list of legal actions\n",
    "        self.moves = self.get_moves()  # list of action space\n",
    "        self.reset(0, 1)\n",
    "\n",
    "    def get_moves(self):\n",
    "        moves = []\n",
    "        directions = [(-2, 0), (2, 0), (0, -2), (0, 2)]\n",
    "        for f in range(49):\n",
    "            if self.board_layout[f] == -1:\n",
    "                continue\n",
    "            x, y = divmod(f, 7)\n",
    "            for dx, dy in directions:\n",
    "                tx, ty = x + dx, y + dy\n",
    "                if 0 <= tx < 7 and 0 <= ty < 7:\n",
    "                    t = tx * 7 + ty\n",
    "                    if self.board_layout[t] != -1:\n",
    "                        m = (x + dx//2) * 7 + y + dy//2\n",
    "                        moves.append([f, m, t])\n",
    "        return np.array(moves, dtype=np.int8)\n",
    "\n",
    "    def get_mask(self):\n",
    "        b = self.board\n",
    "        return (b[self.moves[:, 0]] == 1) & (b[self.moves[:, 1]] == 1) & (b[self.moves[:, 2]] == 0)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.board[self.moves[action]] = [0, 0, 1]\n",
    "        self.remain -= 1\n",
    "\n",
    "        mask = self.get_mask()\n",
    "        self.done = not np.any(mask)\n",
    "\n",
    "        reward = 1.0\n",
    "        if self.done:\n",
    "            if self.remain == 1 and self.board[24] == 1:\n",
    "                reward = 100.0\n",
    "            else:\n",
    "                reward = -5.0 * self.remain\n",
    "\n",
    "        return self.board.copy(), reward, self.done, mask\n",
    "\n",
    "    def reset(self, steps, mode):\n",
    "        if mode == 0:  # start with one piece left in the center and reverse\n",
    "            while True:\n",
    "                self.board = np.full(49, -1, dtype=np.int8)\n",
    "                for idx in self.actions: self.board[idx] = 0\n",
    "                self.board[24] = 1\n",
    "\n",
    "                actual_steps = 0\n",
    "                for _ in range(steps):\n",
    "                    b = self.board\n",
    "                    rev_mask = (b[self.moves[:, 0]] == 0) & (b[self.moves[:, 1]] == 0) & (b[self.moves[:, 2]] == 1)\n",
    "                    possible = np.where(rev_mask)[0]\n",
    "                    if len(possible) == 0: break\n",
    "\n",
    "                    idx = np.random.choice(possible)\n",
    "                    self.board[self.moves[idx, [0, 1, 2]]] = [1, 1, 0]\n",
    "                    actual_steps += 1\n",
    "\n",
    "                if actual_steps == steps:\n",
    "                    break\n",
    "        else:  # take a few random steps at the beginning\n",
    "            self.board = self.board_layout.copy()\n",
    "            for _ in range(steps):\n",
    "                mask = self.get_mask()\n",
    "                actions = np.where(mask)[0]\n",
    "                if len(actions) == 0: break\n",
    "                idx = np.random.choice(actions)\n",
    "                self.board[self.moves[idx]] = [0, 0, 1]\n",
    "\n",
    "        self.remain = np.sum(self.board == 1)\n",
    "        self.done = False\n",
    "        return self.board.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDgluwHGUQu1",
    "trusted": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(49, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 76)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_layer(x)\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "        return value + advantage - advantage.mean(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VR37oe5QUQu1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.n_entries = 0\n",
    "        self.write = 0\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "        self.write = (self.write + 1) % self.capacity\n",
    "        if self.n_entries < self.capacity: self.n_entries += 1\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p\n",
    "        while idx != 0:\n",
    "            idx = (idx - 1) // 2\n",
    "            self.tree[idx] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2 * parent_idx + 1\n",
    "            right_child_idx = left_child_idx + 1\n",
    "            if left_child_idx >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            if v <= self.tree[left_child_idx]:\n",
    "                parent_idx = left_child_idx\n",
    "            else:\n",
    "                v -= self.tree[left_child_idx]\n",
    "                parent_idx = right_child_idx\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[leaf_idx - self.capacity + 1]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ybpAIrLUQu1",
    "trusted": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.episodes = 100000\n",
    "        self.gamma = 0.99\n",
    "        self.lr = 5e-5\n",
    "        self.lr_min = 1e-5\n",
    "        self.batch_size = 256\n",
    "        self.eps = 1.0\n",
    "        self.eps_min = 0.01\n",
    "        self.subtrahend = (self.eps - self.eps_min) / (self.episodes * 0.8)\n",
    "        self.tau = 0.005\n",
    "        self.max_norm = 1.0\n",
    "\n",
    "        self.memory = SumTree(100000)\n",
    "        self.alpha = 0.6\n",
    "        self.beta = 0.4\n",
    "        self.beta_increment = (1.0 - self.beta) / (self.episodes * 0.9)\n",
    "        self.abs_err_upper = 1.0\n",
    "\n",
    "        self.q_network = DQN().to(self.device)\n",
    "        self.target_network = DQN().to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.AdamW(self.q_network.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.episodes, eta_min=self.lr_min)\n",
    "\n",
    "        self.q_history = deque(maxlen=1000)\n",
    "\n",
    "    def update_eps(self):\n",
    "        if self.eps > self.eps_min:\n",
    "            self.eps = max(self.eps_min, self.eps - self.subtrahend)\n",
    "\n",
    "    def act(self, state, mask):\n",
    "        if np.random.random() <= self.eps:\n",
    "            actions = np.where(mask)[0]\n",
    "            return np.random.choice(actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_network(state_t)\n",
    "            mask_t = torch.BoolTensor(mask).to(self.device).unsqueeze(0)\n",
    "            q_values[~mask_t] = float('-inf')\n",
    "            action = q_values.argmax(dim=1).item()  # choose the optimal action's index\n",
    "\n",
    "            self.q_history.append(q_values[0, action].item())\n",
    "\n",
    "            return action\n",
    "\n",
    "    def replay(self):\n",
    "        if self.memory.n_entries < self.batch_size:\n",
    "            return 0.0\n",
    "\n",
    "        indices = []\n",
    "        batch = []\n",
    "        priorities = []\n",
    "        segment = self.memory.total_priority / self.batch_size\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            v = random.uniform(a, b)\n",
    "            idx, p, data = self.memory.get_leaf(v)\n",
    "            priorities.append(p)\n",
    "            indices.append(idx)\n",
    "            batch.append(data)\n",
    "\n",
    "        sampling_probabilities = np.array(priorities) / self.memory.total_priority\n",
    "        is_weights = np.power(self.memory.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weights /= is_weights.max()\n",
    "        is_weights_t = torch.FloatTensor(is_weights).to(self.device)\n",
    "\n",
    "        states = torch.tensor(np.array([e[0] for e in batch]), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor([e[1] for e in batch], dtype=torch.long, device=self.device).unsqueeze(1)\n",
    "        rewards = torch.tensor([e[2] for e in batch], dtype=torch.float32, device=self.device)\n",
    "        next_states = torch.tensor(np.array([e[3] for e in batch]), dtype=torch.float32, device=self.device)\n",
    "        dones = torch.tensor([e[4] for e in batch], dtype=torch.bool, device=self.device)\n",
    "        next_masks = torch.tensor(np.array([e[5] for e in batch]), dtype=torch.bool, device=self.device)\n",
    "\n",
    "        q_values = self.q_network(states)\n",
    "        q_current = q_values.gather(1, actions).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_online = self.q_network(next_states)\n",
    "            next_q_online[~next_masks] = float('-inf')\n",
    "            next_actions = next_q_online.argmax(dim=1, keepdim=True)\n",
    "            next_q_target = self.target_network(next_states)\n",
    "            q_next = next_q_target.gather(1, next_actions).squeeze(1)\n",
    "            q_next[~next_masks.any(dim=1)] = 0.0\n",
    "            q_target = rewards + (self.gamma * q_next * (~dones).float())\n",
    "\n",
    "        td_errors = torch.abs(q_current - q_target).detach().cpu().numpy()\n",
    "        for i in range(self.batch_size):\n",
    "            new_p = np.power(td_errors[i] + 1e-5, self.alpha)\n",
    "            self.memory.update(indices[i], new_p)\n",
    "\n",
    "        loss = (is_weights_t * F.smooth_l1_loss(q_current, q_target, reduction='none')).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=self.max_norm)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        for target_param, param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13o2wBecUQu2",
    "outputId": "8d4d1c65-8f6d-4ab9-f30e-1b99228b816a",
    "trusted": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "env = PegSolitaire()\n",
    "agent = Agent()\n",
    "\n",
    "steps = deque(maxlen=90)\n",
    "scores = deque(maxlen=90)\n",
    "losses = deque(maxlen=90)\n",
    "remains = deque(maxlen=90)\n",
    "least_rem = 1.5\n",
    "\n",
    "for episode in range(1, agent.episodes + 1):\n",
    "    progress = episode / agent.episodes\n",
    "    if progress < 0.7:\n",
    "        mode = 0\n",
    "        max_step = int(3 + 23 * ( (progress / 0.7) ** 2 ))\n",
    "        curr_step = random.randint(max(1, max_step - 5), max_step)\n",
    "        step = curr_step\n",
    "    else:\n",
    "        mode = 1\n",
    "        max_step = int(5 * (1 - (progress - 0.7) / 0.3))\n",
    "        curr_step = random.randint(0, max_step)\n",
    "        step = 31 - curr_step\n",
    "\n",
    "    state = env.reset(curr_step, mode)\n",
    "    mask = env.get_mask()\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if not np.any(mask):\n",
    "            break\n",
    "\n",
    "        action = agent.act(state, mask)\n",
    "        next_state, reward, done, next_mask = env.step(action)\n",
    "\n",
    "        max_p = np.power(agent.abs_err_upper, agent.alpha)\n",
    "        agent.memory.add(max_p, (state, action, reward, next_state, done, next_mask))\n",
    "\n",
    "        if agent.memory.n_entries > agent.batch_size:\n",
    "            loss = agent.replay()\n",
    "            if loss > 0:\n",
    "                losses.append(loss)\n",
    "\n",
    "        state = next_state\n",
    "        mask = next_mask\n",
    "        total_reward += reward\n",
    "\n",
    "    agent.update_eps()  # exploration rate decays linearly with the number of episodes\n",
    "    if len(losses) > 0:\n",
    "        agent.scheduler.step()\n",
    "\n",
    "    scores.append(total_reward)\n",
    "    remains.append(env.remain)\n",
    "    steps.append(step)\n",
    "\n",
    "    if (episode % 100 == 0) and (episode != 0): \n",
    "        print(f\"Episode {episode:6d}  |  Step {np.mean(steps):4.1f}  |  Score {np.mean(scores):7.2f}  |  Loss {np.mean(losses):5.3f}\"\n",
    "            f\"  |  Eps {agent.eps:5.3f}  |  Q {np.mean(agent.q_history):6.2f}  |  Rem {np.mean(remains):4.2f}\")\n",
    "        if (np.mean(steps) > 30.0) and (np.mean(remains) < least_rem):\n",
    "            least_rem = np.mean(remains)\n",
    "            torch.save(agent.q_network.state_dict(), 'model.pth')\n",
    "            \n",
    "print(f\"Least Rem {least_rem:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
